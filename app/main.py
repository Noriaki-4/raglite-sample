from raglite import RAGLiteConfig, Document, insert_documents, rag
from raglite import _rag as rag_module
import litellm
import os
from dataclasses import dataclass
from typing import Any, Callable, List, Optional, Tuple


ToolList = Optional[List[dict[str, Any]]]
ToolChoice = Optional[Any]


@dataclass(frozen=True)
class ModelPolicy:
    name: str
    matcher: Callable[[str], bool]
    configure: Callable[[str], None]
    adjust_tools: Callable[[str, ToolList, ToolChoice], Tuple[ToolList, ToolChoice]]


def _lower(model: str) -> str:
    return (model or "").lower()


def _match_prefix(*prefixes: str) -> Callable[[str], bool]:
    prefixes = tuple(p.lower() for p in prefixes)

    def _matcher(model: str) -> bool:
        lower = _lower(model)
        return any(lower.startswith(prefix) for prefix in prefixes)

    return _matcher


def _identity_adjust(_: str, tools: ToolList, tool_choice: ToolChoice) -> Tuple[ToolList, ToolChoice]:
    return tools, tool_choice


def _anthropic_adjust(_: str, tools: ToolList, tool_choice: ToolChoice) -> Tuple[ToolList, ToolChoice]:
    dummy_tool = {
        "name": "noop",
        "description": "Dummy tool to satisfy Anthropic tool requirement.",
        "input_schema": {
            "type": "object",
            "properties": {},
            "required": [],
        },
    }

    def _convert(tool: dict[str, Any]) -> dict[str, Any]:
        if tool.get("type") == "function":
            func = tool.get("function", {})
            return {
                "name": func.get("name", "function_tool"),
                "description": func.get("description", ""),
                "input_schema": func.get("parameters", {"type": "object", "properties": {}}),
            }
        return tool

    adjusted_tools: ToolList
    if tools:
        adjusted_tools = [_convert(tool) for tool in tools]
    else:
        adjusted_tools = [dummy_tool]

    adjusted_choice = tool_choice if tool_choice is not None else "auto"
    return adjusted_tools, adjusted_choice


def _configure_anthropic(_: str) -> None:
    litellm.modify_params = True


MODEL_POLICIES: List[ModelPolicy] = [
    ModelPolicy(
        name="anthropic",
        matcher=_match_prefix("anthropic.", "claude"),
        configure=_configure_anthropic,
        adjust_tools=_anthropic_adjust,
    ),
    ModelPolicy(
        name="openai",
        matcher=_match_prefix("gpt", "o1", "o3", "text-davinci"),
        configure=lambda _model: None,
        adjust_tools=_identity_adjust,
    ),
    ModelPolicy(
        name="gemini",
        matcher=_match_prefix("gemini", "google"),
        configure=lambda _model: None,
        adjust_tools=_identity_adjust,
    ),
]


def _resolve_model_policy(model: str) -> Optional[ModelPolicy]:
    for policy in MODEL_POLICIES:
        if policy.matcher(model):
            return policy
    return None


_TOOL_PATCH_INSTALLED = False


def _install_tool_policy_hook():
    global _TOOL_PATCH_INSTALLED
    if _TOOL_PATCH_INSTALLED:
        return

    original_get_tools = getattr(rag_module, "_get_tools", None)
    if original_get_tools is None:
        return

    def patched_get_tools(messages, config):
        tools, tool_choice = original_get_tools(messages, config)
        model_name = getattr(config, "llm", "") or ""
        policy = _resolve_model_policy(model_name)
        if policy:
            tools, tool_choice = policy.adjust_tools(model_name, tools, tool_choice)
        return tools, tool_choice

    rag_module._get_tools = patched_get_tools
    _TOOL_PATCH_INSTALLED = True


def _apply_model_policy(model: str) -> str:
    _install_tool_policy_hook()
    policy = _resolve_model_policy(model)
    if policy:
        policy.configure(model)
        return policy.name
    return "generic"

print("=" * 60)
print("Raglite ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ã‚°ãƒ©ãƒ  (Claude LLMä½¿ç”¨)")
print("=" * 60)

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
api_key = os.getenv("ANTHROPIC_API_KEY")
db_path = os.getenv("RAGLITE_DB_PATH", "duckdb:////app/data/raglite.duckdb")
llm_model = os.getenv("RAGLITE_LLM", "claude-3-5-haiku-latest")
embedder_model = os.getenv("RAGLITE_EMBEDDER", "text-embedding-3-small")  # â† ä¿®æ­£

policy_name = _apply_model_policy(llm_model)

# API Keyç¢ºèª
if not api_key:
    print("âš ï¸  è­¦å‘Š: ANTHROPIC_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„: export ANTHROPIC_API_KEY='your-key'")
    exit(1)

# OpenAI API Keyç¢ºèªï¼ˆåŸ‹ã‚è¾¼ã¿ç”¨ï¼‰
openai_key = os.getenv("OPENAI_API_KEY")
if not openai_key and embedder_model.startswith("text-embedding"):
    print("âš ï¸  è­¦å‘Š: OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„: export OPENAI_API_KEY='your-key'")
    exit(1)

print(f"\nâœ“ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {db_path}")
print(f"âœ“ åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«: {embedder_model}")
print(f"âœ“ LLM: {llm_model}")
print(f"âœ“ é©ç”¨ãƒãƒªã‚·ãƒ¼: {policy_name}")

# RAGLiteã®è¨­å®š
config = RAGLiteConfig(
    db_url=db_path,
    embedder=embedder_model,
    llm=llm_model
)

print("âœ“ RAGLiteè¨­å®šå®Œäº†\n")

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ
doc_contents = [
    "Ragliteã¯è»½é‡ãªRAGï¼ˆRetrieval-Augmented Generationï¼‰ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚Pythonã§æ›¸ã‹ã‚Œã¦ãŠã‚Šã€ç°¡å˜ã«ä½¿ãˆã¾ã™ã€‚DuckDBã¾ãŸã¯PostgreSQLã‚’ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã—ã¦ä½¿ç”¨ã§ãã¾ã™ã€‚",
    "Dockerã¯ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚³ãƒ³ãƒ†ãƒŠã¨ã„ã†å˜ä½ã§ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸åŒ–ã—ã€ã©ã“ã§ã‚‚åŒã˜ã‚ˆã†ã«å®Ÿè¡Œã§ãã‚‹æŠ€è¡“ã§ã™ã€‚ã‚³ãƒ³ãƒ†ãƒŠã¯è»½é‡ã§ã€ãƒ›ã‚¹ãƒˆOSã®ã‚«ãƒ¼ãƒãƒ«ã‚’å…±æœ‰ã—ã¾ã™ã€‚",
    "Docker Composeã¯ã€è¤‡æ•°ã®Dockerã‚³ãƒ³ãƒ†ãƒŠã‚’å®šç¾©ã—ã€ä¸€æ‹¬ã§ç®¡ç†ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚YAMLå½¢å¼ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§è¤‡æ•°ã‚µãƒ¼ãƒ“ã‚¹ã‚’ç°¡å˜ã«ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ãã¾ã™ã€‚",
    "Ubuntuã¯ã€Linuxãƒ‡ã‚£ã‚¹ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã®ä¸€ã¤ã§ã€ã‚µãƒ¼ãƒãƒ¼ã‚„ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ç’°å¢ƒã¨ã—ã¦åºƒãä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚Debianç³»ã®ãƒ‡ã‚£ã‚¹ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã§ã€ä½¿ã„ã‚„ã™ã•ã«å®šè©•ãŒã‚ã‚Šã¾ã™ã€‚",
    "DuckDBã¯é«˜é€Ÿãªåˆ†æãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚OLAPå‡¦ç†ã«æœ€é©åŒ–ã•ã‚Œã¦ãŠã‚Šã€è»½é‡ã§çµ„ã¿è¾¼ã¿å¯èƒ½ã§ã™ã€‚SQLiteã®ã‚ˆã†ã«ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ã§å‹•ä½œã—ã€åˆ—æŒ‡å‘ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚",
]

# Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
documents = [Document.from_text(content) for content in doc_contents]

print("ğŸ“ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æŒ¿å…¥ä¸­...")
insert_documents(documents, config=config)
print(f"  âœ“ {len(documents)}å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æŒ¿å…¥ã—ã¾ã—ãŸ\n")

print("=" * 60)

# è³ªå•ãƒªã‚¹ãƒˆ
questions = [
    "Ragliteã¨ã¯ä½•ã§ã™ã‹ï¼Ÿã©ã®ã‚ˆã†ãªç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
    "Dockerã¨ã¯ä½•ã§ã€ãªãœä¾¿åˆ©ãªã®ã§ã™ã‹ï¼Ÿ",
    "DuckDBã®ä¸»ãªåˆ©ç‚¹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
]

# å„è³ªå•ã«å¯¾ã—ã¦RAGã§å›ç­”ç”Ÿæˆ
for question in questions:
    print(f"\nğŸ’¬ è³ªå•: {question}")
    print("-" * 60)
    
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’ä½œæˆ
    messages = [{"role": "user", "content": question}]
    
    # æ¤œç´¢ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã‚’æ ¼ç´
    chunk_spans = []
    
    # RAGå®Ÿè¡Œ: æ¤œç´¢ + Claude LLMã§å›ç­”ç”Ÿæˆ
    print("ğŸ¤– Claudeå›ç­”:")
    stream = rag(
        messages, 
        on_retrieval=lambda x: chunk_spans.extend(x), 
        config=config
    )
    
    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›
    for update in stream:
        print(update, end="", flush=True)
    
    print("\n")
    print(f"ğŸ“š å‚ç…§ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(chunk_spans)}")
    print("-" * 60)

print("\n" + "=" * 60)
print("ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†")
print("=" * 60)
